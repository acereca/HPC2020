\documentclass[]{scrartcl}
\usepackage{Preamble}

\setcounter{section}{6}
\newcommand{\exercise}{Exercise \thesection}
\newcommand{\duedate}{2021-01-18, 23:59}

\begin{document}
\section*{\exercise}

To compile: unzip our uploaded code, and run \verb|make| inside \verb|code/|.
The slurm scripts are stored inside \verb|code/slurm/|.

To debug: run the debug outputs (\verb|*.dbg|) and attach gdb to respective pids

\subsection{Heat Relaxation II --- Parallel Implementation}\label{ssec:impl}
\subsection{Heat Relaxation II --- Experiments}

The implementation in \autoref{ssec:impl} resulted in the values below (\autoref{tabl:heat_t}, \autoref{tabl:heat_s}, and \autoref{tabl:heat_e}).
We choose the by-slot-Mapping (default of mpirun) as to reduce the number of hops between nodes.
This Mapping starts filling a node's possible slots with ranks until full and then continues with another node.
This results in rank 3 and 4 and ranks 7 and 8 communication between nodes (for 9 or more ranks).

\begin{table}[ht]
  \caption{Time [$\mu$s] / iteration}\label{tabl:heat_t}
  \input{data/heat_t}
\end{table}

\begin{table}[ht]
  \caption{Speedup}\label{tabl:heat_s}
  \input{data/heat_s}
\end{table}

\begin{table}[ht]
  \caption{Efficiency}\label{tabl:heat_e}
  \input{data/heat_e}
\end{table}

\begin{itemize}
  \item A speedup is observed, that correlates to the number of jobs (i.e.\ for 10 jobs we reach a speedup o approx 10, for a sufficiently large problem size)
  \item For problem sizes too small, performance drops, due to the communication overhead dominating
  \item Additionally super linear speedups were observed, probably due to better cache utilization.
\end{itemize}

\subsection{Heat Relaxation II --- Tracing}
\subsubsection{Expectations From Tracing}
Since the stencil is a memory and computation heavy tasks we would expect most time being spend
inside the stencil computation i.e.\ \texttt{relaxation\_step()} function in our case.
Since only halo values are sent among tasks we don't expect communication to be as intensive.

\subsubsection{Expectations From Experiments}
The performed experiments meet our expectations.
Smaller workloads get worse with rising number of tasks since communication overhead increases.
For larger workloads we reach speedups approximately proportional to the number of tasks.

\subsubsection{Tracing}
Using \emph{scorep} to trace our application confirms our initial expectation:
\begin{figure}[htpb]
	\centering
	\includegraphics[width=1\linewidth]{./img/scorep_128_100.png}
	\caption{Tracing result for problem size of 128x128. Most time (around 50\%) is spent in MPI as expected. }%
	\label{fig:./img/scorep_128_100}
\end{figure}

\begin{figure}[htpb]
	\centering
	\includegraphics[width=1\linewidth]{./img/scorep_2048_10.png}
	\caption{Using a larger problem size of 2048x2048 MPI time becomes almost negligible (around 4\%)}%
	\label{fig:./img/scorep_2048_10}
\end{figure}
For large problems the number of iterations does not seem to change the result too much.

Looking at user time we can also see that most of the time is spent accessing vector items:
\begin{figure}[htpb]
	\centering
	\includegraphics[width=1\linewidth]{./img/scorep_funcs.png}
	\caption{We can see that 25\% of time is spent \texttt{relaxation\_step()} doing actual computation. 42\% of the time is spent fetching elements from the grid}%
	\label{fig:/img/scorep_funcs}

\end{figure}
\subsubsection{Insights from tracing}
Using this information we could optimize memory access further (speed up index calculation).

\end{document}
